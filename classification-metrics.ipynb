{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, zero_one_loss, jaccard_score, confusion_matrix, \\\n    precision_score, recall_score, fbeta_score\n\n\n# For reproducibility\nnp.random.seed(1000)\n\nnb_samples = 500","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dataset\nX, Y = make_classification(n_samples=nb_samples, n_features=2, n_informative=2, n_redundant=0,\n                               n_clusters_per_class=1)\n\n# Split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and train logistic regressor\nlr = LogisticRegression()\nlr.fit(X_train, Y_train)\n\nprint('Accuracy score: %.3f' % accuracy_score(Y_test, lr.predict(X_test)))\nprint('Zero-one loss (normalized): %.3f' % zero_one_loss(Y_test, lr.predict(X_test)))\nprint('Zero-one loss (unnormalized): %.3f' % zero_one_loss(Y_test, lr.predict(X_test), normalize=False))\nprint('Jaccard similarity score: %.3f' % jaccard_score(Y_test, lr.predict(X_test)))","execution_count":8,"outputs":[{"output_type":"stream","text":"Accuracy score: 0.992\nZero-one loss (normalized): 0.008\nZero-one loss (unnormalized): 1.000\nJaccard similarity score: 0.986\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"It seesm that zero-one loss: misclassification/total # of samples, ***without differentiating the types of error***. Another detail: ***\"If normalize is True, return the fraction of misclassifications (float), else it returns the number of misclassifications (int). The best performance is 0.\"***"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute confusion matrix\ncm = confusion_matrix(y_true=Y_test, y_pred=lr.predict(X_test))\nprint('Confusion matrix:')\nprint(cm)\n\nprint('Precision score: %.3f' % precision_score(Y_test, lr.predict(X_test)))\nprint('Recall score: %.3f' % recall_score(Y_test, lr.predict(X_test)))\nprint('F-Beta score (1): %.3f' % fbeta_score(Y_test, lr.predict(X_test), beta=1))\nprint('F-Beta score (0.75): %.3f' % fbeta_score(Y_test, lr.predict(X_test), beta=0.75))\nprint('F-Beta score (1.25): %.3f' % fbeta_score(Y_test, lr.predict(X_test), beta=1.25))","execution_count":6,"outputs":[{"output_type":"stream","text":"Confusion matrix:\n[[56  1]\n [ 0 68]]\nPrecision score: 0.986\nRecall score: 1.000\nF-Beta score (1): 0.993\nF-Beta score (0.75): 0.991\nF-Beta score (1.25): 0.994\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"confusion_matrix: By definition a confusion matrix C is such that Cij is equal to the number of observations known to be in group i and predicted to be in group j.\\\n\\\nThus in binary classification, the count of true negatives is C<sub>0,0</sub>, false negatives is C<sub>1,0</sub>, true positives is C<sub>1,1</sub> and false positives is C<sub>0,1</sub>(So in both axes, negative comes first by default).\\\n\\\nIf you don't flip the confusion matrix, but want to get the same measures, it's necessary to add the ***pos_label=0*** parameter to all metric score functions."},{"metadata":{},"cell_type":"markdown","source":"***Precision = TP/(TP + FP)\\\n\\\nRecall = TP/(TP + FN)\\\n\\\nJaccard Similarity Score = TP/(TP + FP + FN)\\\n\\\nAccuracy = (TP + TN)/(TP + FP + FN + TN)***\\\n\\\nTherefore, for what it's worth, Jaccard similarity score should be smaller than any of the other three.\\ "},{"metadata":{},"cell_type":"markdown","source":"\\\\(F_{\\beta} = (\\beta^2 + 1)*\\frac{Precision * Recall}{\\beta^2Precision + Recall}\\\\)\n\\\n\\\nOr, it can also be expressed as, \\\\(F_{\\beta} = \\frac{Precision * Recall}{\\frac{\\beta^2}{\\beta^2 + 1}Precision + \\frac{1}{\\beta^2 + 1} Recall}\\\\)\\\n\\\nThe most common F score is the case of \\\\(\\beta = 1\\\\), where equal weights are given to precision and recall. The bigger the \\\\(\\beta\\\\), the bigger emphasis will be placed on precision."},{"metadata":{},"cell_type":"markdown","source":"The highest score is achieved by giving more importance to precision (which is higher), while the least one corresponds to a recall predominance. \\\\(F_{\\beta}\\\\) is\nhence useful to have a compact picture of the accuracy as a trade-off between high precision and a limited number of false negatives."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}